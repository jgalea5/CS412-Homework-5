{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project by Alexis U. and Jose G.\n",
    "\n",
    "Our problem for project 5 is about working for a non-profit that is trying to recruit student volunteers to help with Alzheimer’s patients. Our job is to predict how suitable a person is for this volunteering job. We are using the young people survey dataset from Kaggle.com provided in the HW5 write up. The first thing we did was to convert all the answers that are strings into either 1 to 2 or 1 to 5 integers depending on how many possible answers the person could answer. The reason why we did this is because you can’t have integers and strings in your data. It makes sense to convert the strings to integers for this problem. We also added a response to the empty answers because it makes more sense to add a random number instead of adding a zero or dropping the rows that don’t have an answer. After we are done processing the data then we split the data into 70% training, 10% development, and 20% test. We save these into their own separate files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will do the entire preprocess to get the data ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening file...\n",
      "data stored...\n",
      "converting categorical data...\n",
      "data converted...\n",
      "new file \"formatted_responses.csv\" created\n",
      "splitting data into train/dev/test files\n",
      "creating responses.tr...\n",
      "file created...\n",
      "creating responses.de...\n",
      "file created...\n",
      "creating responses.te...\n",
      "file created...\n",
      "data split completed...\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import *\n",
    "process_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will split the data for training, development, and testing. Now will print our results to for training, development, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5. 3. 2. ... 1. 2. 2.]\n",
      " [4. 4. 2. ... 1. 1. 2.]\n",
      " [5. 5. 2. ... 1. 1. 2.]\n",
      " ...\n",
      " [5. 5. 5. ... 2. 1. 2.]\n",
      " [5. 3. 4. ... 1. 1. 2.]\n",
      " [5. 3. 3. ... 1. 2. 1.]]\n",
      "[[5. 2. 3. ... 1. 1. 1.]\n",
      " [5. 3. 4. ... 1. 1. 2.]\n",
      " [5. 4. 2. ... 2. 1. 2.]\n",
      " ...\n",
      " [4. 3. 1. ... 2. 1. 2.]\n",
      " [5. 3. 3. ... 1. 1. 2.]\n",
      " [5. 5. 4. ... 1. 2. 1.]]\n",
      "[[5. 5. 5. ... 1. 1. 2.]\n",
      " [5. 2. 1. ... 1. 1. 2.]\n",
      " [4. 3. 3. ... 1. 1. 2.]\n",
      " ...\n",
      " [5. 3. 4. ... 1. 1. 2.]\n",
      " [5. 3. 5. ... 1. 1. 2.]\n",
      " [5. 4. 4. ... 1. 1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import *\n",
    "Xtr, Ytr = feature_label_split(\"data/responses.tr\")\n",
    "Xde, Yde = feature_label_split(\"data/responses.de\")\n",
    "Xte, Yte = feature_label_split(\"data/responses.te\")\n",
    "print(Xtr)\n",
    "print(Xde)\n",
    "print(Xte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start making any modifications to the classifier or doing anything with the hyperparameters, we need some baseline classifiers to see the results. We will compare the baseline classifiers by comparing the accuracy of each one against each other to see which one is performing better. The code below should give us the accurracy for the most frequent classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from pylab import *\n",
    "from baselines import *\n",
    "\n",
    "# baseline classifier\n",
    "# most_frequent class\n",
    "base = most_frequent()\n",
    "base.train(Xtr, Ytr)\n",
    "s3 = base.predict_all(Xde)\n",
    "\n",
    "print(mean(s3==Yde))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we write the code for a simple decision tree classifier with max depth of 6. The accurracy of this should be around 0.74-0.75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7450980392156863\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from pylab import *\n",
    "\n",
    "# decision tree classifier\n",
    "classifier = tree.DecisionTreeClassifier(max_depth=6)\n",
    "classifier.fit(Xtr, Ytr)\n",
    "s1 = classifier.predict(Xde)\n",
    "print(mean(s1==Yde))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we write the code for a SVC with the kernel set as poly. The accuracy for the SVC should be around 0.70-0.71."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7058823529411765\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from pylab import *\n",
    "\n",
    "# support vector machine\n",
    "classifier = SVC(kernel='poly')\n",
    "classifier.fit(Xtr, Ytr)\n",
    "svc_class = classifier.predict(Xde)\n",
    "print(mean(svc_class==Yde))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we write the code for a percentron with a random_state = 0 and we set max_iter to 850. This will give us an accurracy of around 0.68-0.69."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6862745098039216\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from pylab import *\n",
    "\n",
    "# perceptron model\n",
    "classifier = Perceptron(random_state=0, max_iter= 850)\n",
    "classifier.fit(Xtr, Ytr)\n",
    "percept_model = classifier.predict(Xde)\n",
    "print(mean(percept_model==Yde))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
